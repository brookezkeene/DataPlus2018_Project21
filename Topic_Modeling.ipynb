{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A team of students will work with Duke’s Office of Information Technology to conceptualize and potentially develop an “e-advisor” program that will help students navigate, augment, and map their way through Duke’s co-curricular ecosystem. The team of students will identify available data, programs and resources, define learning objectives, recommend common pathways and create a storyboard of the program building out a “master narrative” experience and prototype the branching and decision engine. Students will work with de-identified registration and advising data in a secure environment, have access to the analytics tools used in OIT, and will have an opportunity for exploration of the data in consultation with OIT and data analytics professionals.\n",
      "\n",
      "['lines', 'string', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Data+ Web Scraping Test\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Initialize variables (only for Data+ 2018 websites - figure out for other)\n",
    "sites = [\"https://bigdata.duke.edu/projects/co-curricular-technology-pathways-e-advisor\"]\n",
    "# Program descriptions\n",
    "descriptions = []\n",
    "# Final list of nouns\n",
    "terms = []\n",
    "\n",
    "# Access websites and extract information\n",
    "for i in sites:\n",
    "    link = i\n",
    "    page = requests.get(link)\n",
    "    # Checks if page downloaded successfully\n",
    "    if page.status_code != 200:\n",
    "        break\n",
    "        \n",
    "# Extracts HTML content on page ('lxml' works too)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "# Get the descriptions\n",
    "title = soup.find(text = 'Project Summary')\n",
    "body = title.next.next\n",
    "descriptions.append(body.get_text())\n",
    "\n",
    "# Cleaning documents\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i in descriptions:\n",
    "    print(i)\n",
    "    raw = i.lower()\n",
    "    \n",
    "# Tokenization\n",
    "#tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "# Create list with all words and POS \n",
    "#print(nltk.pos_tag(tokens))\n",
    "#print(\"\\n\")\n",
    "\n",
    "# function to test if something is a noun\n",
    "lines = 'lines is some string of words'\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "# do the nlp stuff\n",
    "tokenized = nltk.word_tokenize(lines)\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test TextBlob package\n",
      "['natural language processing', 'nlp', 'computer science', 'artificial intelligence', 'computational linguistics', 'inter actions'] \n",
      "\n",
      "Use TextBlob to extract tags\n",
      "['duke ’ s office', 'information technology', '“ e-advisor ” program', 'students navigate', 'duke ’ s co-curricular ecosystem', 'available data', 'learning objectives', 'common pathways', 'program building', '“ master narrative ” experience', 'decision engine', 'analytics tools', 'data analytics professionals'] \n",
      "\n",
      "Get rid of stop words using stop words package\n",
      "['duke ’ s office', 'information technology', '“ e-advisor ” program', 'students navigate', 'duke ’ s co-curricular ecosystem', 'available data', 'learning objectives', 'common pathways', 'program building', '“ master narrative ” experience', 'decision engine', 'analytics tools', 'data analytics professionals'] \n",
      "\n",
      "Get rid of manually selected useless words\n",
      "['duke ’ s office', 'information technology', '“ e-advisor ” program', 'students navigate', 'duke ’ s co-curricular ecosystem', 'available data', 'learning objectives', 'common pathways', 'program building', '“ master narrative ” experience', 'decision engine', 'analytics tools', 'data analytics professionals'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TextBlob \n",
    "# Function: gets all words and phrases associated with nouns\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Testing tokenization\n",
    "print(\"Test TextBlob package\")\n",
    "txt = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the inter\n",
    "actions between computers and human (natural) languages.\"\"\"\n",
    "blobTest = TextBlob(txt)\n",
    "print(blobTest.noun_phrases, \"\\n\")\n",
    "\n",
    "# Use TextBlob to extract tags\n",
    "print(\"Use TextBlob to extract tags\")\n",
    "blob = TextBlob(raw)\n",
    "print(blob.noun_phrases, \"\\n\")\n",
    "\n",
    "# Clean the list\n",
    "# Get rid of stop words using stop words package\n",
    "print(\"Get rid of stop words using stop words package\")\n",
    "en_stop = get_stop_words('en')\n",
    "words = [i for i in blob.noun_phrases if not i in en_stop]\n",
    "print(words, \"\\n\")    \n",
    "    \n",
    "# Get rid of manually selected useless words\n",
    "print(\"Get rid of manually selected useless words\")\n",
    "new_words = []\n",
    "a = [\"team\",\"students\",\"opportunities\", \"work\", \"access\", \"s\"]\n",
    "for n in a:\n",
    "    new_words = [x for x in words if x != n]\n",
    "    words = new_words   \n",
    "print(new_words, \"\\n\")\n",
    "terms.append(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.077*\"program building\" + 0.077*\"“ e-advisor ” program\" + 0.077*\"information technology\" + 0.077*\"students navigate\" + 0.077*\"duke ’ s office\"'), (1, '0.077*\"students navigate\" + 0.077*\"data analytics professionals\" + 0.077*\"program building\" + 0.077*\"learning objectives\" + 0.077*\"available data\"'), (2, '0.077*\"duke ’ s office\" + 0.077*\"decision engine\" + 0.077*\"common pathways\" + 0.077*\"information technology\" + 0.077*\"duke ’ s co-curricular ecosystem\"')]\n"
     ]
    }
   ],
   "source": [
    "# LDA\n",
    "\n",
    "# Construct a document-term matrix\n",
    "dictionary = corpora.Dictionary(terms)\n",
    "corpus = [dictionary.doc2bow(t) for t in terms]\n",
    "\n",
    "# Apply the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics= 3, id2word = dictionary, passes = 50)\n",
    "print(lda_model.print_topics(num_topics = 3, num_words = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rake'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9c1eb0c0f13a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RAKE-tutorial'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnlp_rake\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstoppath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RAKE-tutorial/data/stoplists/SmartStoplist.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nlp_rake-1.0-py3.6.egg/nlp_rake/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrake\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rake'"
     ]
    }
   ],
   "source": [
    "# RAKE 1\n",
    "import sys\n",
    "sys.path.append('RAKE-tutorial')\n",
    "from nlp_rake import rake\n",
    "\n",
    "stoppath = 'RAKE-tutorial/data/stoplists/SmartStoplist.txt'\n",
    "\n",
    "rake_object = rake.Rake(stoppath, 4, 3, 6)\n",
    "\n",
    "sample_file = open(\"/Users/derekliu/Desktop//Data+/21\", 'r', encoding=\"iso-8859-1\")\n",
    "text = sample_file.read()\n",
    "keywords = rake_object.run(text)\n",
    "\n",
    "# 3. print results\n",
    "print(\"Keywords:\", keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“ master narrative ” experience',\n",
       " 'advisor ” program',\n",
       " 'recommend common pathways',\n",
       " 'define learning objectives',\n",
       " 'analytics tools used',\n",
       " 'identify available data',\n",
       " 'data analytics professionals',\n",
       " 'help students navigate',\n",
       " '“ e',\n",
       " 'program building',\n",
       " 'advising data',\n",
       " 'secure environment',\n",
       " 'potentially develop',\n",
       " 'information technology',\n",
       " 'identified registration',\n",
       " 'duke ’',\n",
       " 'decision engine',\n",
       " 'curricular ecosystem',\n",
       " 'data',\n",
       " 'students',\n",
       " 'work',\n",
       " 'way',\n",
       " 'team',\n",
       " 'storyboard',\n",
       " 'resources',\n",
       " 'prototype',\n",
       " 'programs',\n",
       " 'opportunity',\n",
       " 'oit',\n",
       " 'office',\n",
       " 'map',\n",
       " 'exploration',\n",
       " 'de',\n",
       " 'create',\n",
       " 'consultation',\n",
       " 'conceptualize',\n",
       " 'co',\n",
       " 'branching',\n",
       " 'augment',\n",
       " 'access']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAKE 2\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters by default\n",
    "r = Rake()\n",
    "\n",
    "# Extraction given the text.\n",
    "myText = '''A team of students will work with Duke’s Office of Information Technology to \n",
    "conceptualize and potentially develop an “e-advisor” program that will help students navigate,\n",
    "augment, and map their way through Duke’s co-curricular ecosystem. The team of students will \n",
    "identify available data, programs and resources, define learning objectives, recommend common \n",
    "pathways and create a storyboard of the program building out a “master narrative” experience\n",
    "and prototype the branching and decision engine. Students will work with de-identified registration \n",
    "and advising data in a secure environment, have access to the analytics tools used in OIT, and \n",
    "will have an opportunity for exploration of the data in consultation with OIT and data analytics professionals.'''\n",
    "r.extract_keywords_from_text(myText)\n",
    "\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A team of students will work with Duke’s Office of Information Technology to conceptualize and potentially develop an “e-advisor” program that will help students navigate, augment, and map their way through Duke’s co-curricular ecosystem. The team of students will identify available data, programs and resources, define learning objectives, recommend common pathways and create a storyboard of the program building out a “master narrative” experience and prototype the branching and decision engine. Students will work with de-identified registration and advising data in a secure environment, have access to the analytics tools used in OIT, and will have an opportunity for exploration of the data in consultation with OIT and data analytics professionals.\n",
      "\n",
      "['team', 'students', 'Duke', '’', 'Office', 'Information', 'Technology', '”', 'program', 'students', 'augment', 'way', 'Duke', '’', 'ecosystem', 'team', 'students', 'data', 'programs', 'resources', 'define', 'objectives', 'pathways', 'storyboard', 'program', 'building', 'master', '”', 'experience', 'branching', 'decision', 'engine', 'Students', 'registration', 'data', 'secure', 'environment', 'access', 'analytics', 'tools', 'OIT', 'opportunity', 'exploration', 'data', 'consultation', 'OIT', 'data', 'analytics', 'professionals']\n"
     ]
    }
   ],
   "source": [
    "# nltk select nouns\n",
    "\n",
    "# Cleaning documents\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i in descriptions:\n",
    "    print(i)\n",
    "    raw = i.lower()\n",
    "\n",
    "# Tokenization\n",
    "#tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "# Create list with all words and POS \n",
    "#print(nltk.pos_tag(tokens))\n",
    "#print(\"\\n\")\n",
    "\n",
    "# Find all nouns\n",
    "# lines = desc[3]\n",
    "lines = 'A team of students will work with Duke’s Office of Information Technology to conceptualize and potentially develop an “e-advisor” program that will help students navigate, augment, and map their way through Duke’s co-curricular ecosystem. The team of students will identify available data, programs and resources, define learning objectives, recommend common pathways and create a storyboard of the program building out a “master narrative” experience and prototype the branching and decision engine. Students will work with de-identified registration and advising data in a secure environment, have access to the analytics tools used in OIT, and will have an opportunity for exploration of the data in consultation with OIT and data analytics professionals.'\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "tokenized = nltk.word_tokenize(lines)\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['team', 'students', 'work', 'duke', 'office', 'information', 'technology', 'conceptualize', 'potentially', 'develop', 'eadvisor', 'program', 'help', 'students', 'navigate', 'augment', 'map', 'way', 'duke', 'cocurricular', 'ecosystem', 'team', 'students', 'identify', 'available', 'data', 'programs', 'resources', 'define', 'learning', 'objectives', 'recommend', 'common', 'pathways', 'create', 'storyboard', 'program', 'building', 'master', 'narrative', 'experience', 'prototype', 'branching', 'decision', 'engine', 'students', 'work', 'deidentified', 'registration', 'advising', 'data', 'secure', 'environment', 'access', 'analytics', 'tools', 'used', 'oit', 'opportunity', 'exploration', 'data', 'consultation', 'oit', 'data', 'analytics', 'professionals']\n",
      "['team', 'students', 'office', 'information', 'technology', 'conceptualize', 'eadvisor', 'program', 'help', 'students', 'map', 'way', 'ecosystem', 'team', 'students', 'data', 'programs', 'resources', 'objectives', 'pathways', 'program', 'building', 'master', 'experience', 'prototype', 'decision', 'engine', 'students', 'registration', 'data', 'secure', 'environment', 'access', 'analytics', 'tools', 'opportunity', 'exploration', 'data', 'consultation', 'data', 'analytics', 'professionals']\n"
     ]
    }
   ],
   "source": [
    "# More advanced nltk cleaning text and extracting nouns\n",
    "\n",
    "# Clean Descriptions\n",
    "# load data\n",
    "# filename = '/Users/derekliu/Desktop/DukeGroups.txt'\n",
    "filename = '/Users/derekliu/Desktop/Data+/21.txt'\n",
    "\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])\n",
    "nouns = [word for (word, pos) in nltk.pos_tag(words) if is_noun(pos)] \n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add more words to stop_words to make it more accurate\n",
    "\n",
    "# Build word filter\n",
    "trivial = [\"team\",\"students\",\"opportunities\", \"work\", \"access\", \"duke\", \"student\", \"effort\", \"consumer\", \"consumers\", \"club\",\n",
    "           \"home\", \"school\", \"schools\", \"act\", \"day\", \"seafood\", \"africa\", \"anthropology\", \"faculty\", \"goal\", \"group\",\n",
    "          \"skills\", \"consumption\", \"hosting\", \"graduate\", \"b\", \"majors\", \"major\", \"session\", \"sessions\", \"articles\",\n",
    "          \"article\", \"department\", \"departments\", \"year\", \"members\", \"field\", \"fields\", \"activity\", \"activities\", \"organization\",\n",
    "          \"organizations\", \"today\", \"tomorrow\", \"events\", \"event\", \"outing\", \"outings\",\"idea\", \"ideas\", \"talks\", \"university\", \"universities\"]\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# filter out stop words using our own library\n",
    "for word in trivial:\n",
    "    stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['team',\n",
       " 'students',\n",
       " 'office',\n",
       " 'information',\n",
       " 'technology',\n",
       " 'conceptualize',\n",
       " 'eadvisor',\n",
       " 'program',\n",
       " 'help',\n",
       " 'students',\n",
       " 'map',\n",
       " 'way',\n",
       " 'ecosystem',\n",
       " 'team',\n",
       " 'students',\n",
       " 'data',\n",
       " 'programs',\n",
       " 'resources',\n",
       " 'objectives',\n",
       " 'pathways',\n",
       " 'program',\n",
       " 'building',\n",
       " 'master',\n",
       " 'experience',\n",
       " 'prototype',\n",
       " 'decision',\n",
       " 'engine',\n",
       " 'students',\n",
       " 'registration',\n",
       " 'data',\n",
       " 'secure',\n",
       " 'environment',\n",
       " 'access',\n",
       " 'analytics',\n",
       " 'tools',\n",
       " 'opportunity',\n",
       " 'exploration',\n",
       " 'data',\n",
       " 'consultation',\n",
       " 'data',\n",
       " 'analytics',\n",
       " 'professionals']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (1, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (2, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (3, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (4, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (5, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (6, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (7, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (8, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (9, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (10, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (11, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (12, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (13, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (14, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (15, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (16, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (17, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (18, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (19, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (20, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (21, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (22, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (23, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (24, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (25, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (26, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (27, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (28, '0.033*\"data\" + 0.033*\"professionals\" + 0.033*\"program\" + 0.033*\"prototype\" + 0.033*\"pathways\"'), (29, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (30, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (31, '0.113*\"data\" + 0.057*\"analytics\" + 0.057*\"program\" + 0.029*\"programs\" + 0.029*\"opportunity\"'), (32, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (33, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (34, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (35, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (36, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (37, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (38, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (39, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (40, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (41, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (42, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (43, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (44, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (45, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (46, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (47, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (48, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"'), (49, '0.033*\"professionals\" + 0.033*\"opportunity\" + 0.033*\"programs\" + 0.033*\"pathways\" + 0.033*\"registration\"')]\n"
     ]
    }
   ],
   "source": [
    "# DukeGroups LDA method, need to be in sentence form\n",
    "# Construct a document-term matrix, input needs to be double bracket\n",
    "dic = []\n",
    "\n",
    "for x in range (0, 1): #len(desc)-1\n",
    "#     singleDesc = desc[x]\n",
    "    singleDesc = lines\n",
    "    ### Clean text\n",
    "    # split into words\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(singleDesc)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    import string\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words using nltk\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    # extract all nouns\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(words) if is_noun(pos)] \n",
    "    dic.append(nouns)\n",
    "dictionary = corpora.Dictionary(dic)\n",
    "corpus = [dictionary.doc2bow(t) for t in dic]\n",
    "\n",
    "# # Apply the LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics= 50, id2word = dictionary, passes = 100)\n",
    "print(lda_model.print_topics(num_topics = 50, num_words = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
